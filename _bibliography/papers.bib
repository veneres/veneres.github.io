---
---

@inproceedings{burch_eyeclouds_2019,
	address = {Shanghai China},
	title = {{EyeClouds}: {A} {Visualization} and {Analysis} {Tool} for {Exploring} {Eye} {Movement} {Data}},
	isbn = {978-1-4503-7626-6},
	shorttitle = {{EyeClouds}},
	url = {https://dl.acm.org/doi/10.1145/3356422.3356423},
	doi = {10.1145/3356422.3356423},
	abstract = {In this paper, we discuss and evaluate the advantages and disadvantages of several techniques to visualize and analyze eye movement data tracked and recorded from public transport map viewers in a formerly conducted eye tracking experiment. Such techniques include heat maps and gaze stripes. To overcome the disadvantages and improve the effectiveness of those techniques, we present a viable solution that makes use of existing techniques such as heat maps and gaze stripes, as well as attention clouds which are inspired by the general concept of word clouds. We also develop a web application with interactive attention clouds, named the EyeCloud, to put theory into practice. The main objective of this paper is to help public transport map designers and producers gain feedback and insights on how the current design of the map can be further improved, by leveraging on the visualization tool. In addition, this visualization tool, the EyeCloud, can be easily extended to many other purposes with various types of data. It could be possibly applied to entertainment industries, for instance, to track the attention of the film audiences in order to improve the advertisements.},
	language = {en},
	urldate = {2021-10-25},
	booktitle = {Proceedings of the 12th {International} {Symposium} on {Visual} {Information} {Communication} and {Interaction}},
	publisher = {ACM},
	author = {Burch, Michael and Veneri, Alberto and Sun, Bangjie},
	month = sep,
	year = {2019},
	pages = {1--8},
}


@article{burch_exploring_2020,
	title = {Exploring eye movement data with image-based clustering},
	volume = {23},
	issn = {1343-8875, 1875-8975},
	url = {http://link.springer.com/10.1007/s12650-020-00656-9},
	doi = {10.1007/s12650-020-00656-9},
	abstract = {In this article, we describe a new feature for exploring eye movement data based on image-based clustering. To reach this goal, visual attention is taken into account to compute a list of thumbnail images from the presented stimulus. These thumbnails carry information about visual scanning strategies, but showing them just in a space-ﬁlling and unordered fashion does not support the detection of patterns over space, time, or study participants. In this article, we present an enhancement of the EyeCloud approach that is based on standard word cloud layouts adapted to image thumbnails by exploiting image information to cluster and group the thumbnails that are visually attended. To also indicate the temporal sequence of the thumbnails, we add color-coded links and further visual features to dig deeper in the visual attention data. The usefulness of the technique is illustrated by applying it to eye movement data from a formerly conducted eye tracking experiment investigating route ﬁnding tasks in public transport maps. Finally, we discuss limitations and scalability issues of the approach.},
	language = {en},
	number = {4},
	urldate = {2021-01-08},
	journal = {Journal of Visualization},
	author = {Burch, Michael and Veneri, Alberto and Sun, Bangjie},
	month = aug,
	year = {2020},
	pages = {677--694},
}


@misc{lucchese_gam_2023,
	title = {{GAM} {Forest} {Explanation}},
	url = {https://openproceedings.org/2023/conf/edbt/paper-207.pdf},
	doi = {10.48786/EDBT.2023.14},
	abstract = {Most accurate machine learning models unfortunately produce black-box predictions, for which it is impossible to grasp the internal logic that leads to a speci c decision. Unfolding the logic of such black-box models is of increasing importance, especially when they are used in sensitive decision-making processes. In this work we focus on forests of decision trees, which may include hundreds to thousands of decision trees to produce accurate predictions. Such complexity raises the need of developing explanations for the predictions generated by large forests. We propose a post hoc explanation method of large forests, named GAM-based Explanation of Forests (GEF), which builds a Generalized Additive Model (GAM) able to explain, both locally and globally, the impact on the predictions of a limited set of features and feature interactions. We evaluate GEF over both synthetic and real-world datasets and show that GEF can create a GAM model with high delity by analyzing the given forest only and without using any further information, not even the initial training dataset.},
	language = {en},
	urldate = {2023-04-23},
	publisher = {OpenProceedings.org},
	author = {Lucchese, Claudio and Orlando, Salvatore and Perego, Raffaele and Veneri, Alberto},
	year = {2023},
	doi = {10.48786/EDBT.2023.14},
	keywords = {Database Technology},
}


@inproceedings{lucchese_ilmart_2022,
	address = {New York, NY, USA},
	series = {{SIGIR} '22},
	title = {{ILMART}: {Interpretable} {Ranking} with {Constrained} {LambdaMART}},
	isbn = {978-1-4503-8732-3},
	shorttitle = {{ILMART}},
	url = {https://dl.acm.org/doi/10.1145/3477495.3531840},
	doi = {10.1145/3477495.3531840},
	abstract = {Interpretable Learning to Rank (LtR) is an emerging field within the research area of explainable AI, aiming at developing intelligible and accurate predictive models. While most of the previous research efforts focus on creating post-hoc explanations, in this paper we investigate how to train effective and intrinsically-interpretable ranking models. Developing these models is particularly challenging and it also requires finding a trade-off between ranking quality and model complexity. State-of-the-art rankers, made of either large ensembles of trees or several neural layers, exploit in fact an unlimited number of feature interactions making them black boxes. Previous approaches on intrinsically-interpretable ranking models address this issue by avoiding interactions between features thus paying a significant performance drop with respect to full-complexity models. Conversely, ILMART, our novel and interpretable LtR solution based on LambdaMART, is able to train effective and intelligible models by exploiting a limited and controlled number of pairwise feature interactions. Exhaustive and reproducible experiments conducted on three publicly-available LtR datasets show that ILMART outperforms the current state-of-the-art solution for interpretable ranking of a large margin with a gain of nDCG of up to 8\%.},
	urldate = {2023-04-23},
	booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Lucchese, Claudio and Nardini, Franco Maria and Orlando, Salvatore and Perego, Raffaele and Veneri, Alberto},
	month = jul,
	year = {2022},
	keywords = {interpretable boosting, interpretable ranking, lambdamart},
	pages = {2255--2259},
}



